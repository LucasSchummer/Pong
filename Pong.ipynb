{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541203a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import random\n",
    "import glob\n",
    "import imageio\n",
    "import cv2 as cv\n",
    "import os\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdc8d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1488346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map = {\n",
    "    0 : 0,\n",
    "    1 : 2,\n",
    "    2 : 3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77d70a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_average_return(Q, n_episodes, epsilon):\n",
    "\n",
    "    returns = []\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        # Skip first frame (different color)\n",
    "        frame, _ = env.reset()\n",
    "        _ = env.step(0)\n",
    "\n",
    "        frame, reward, terminated, truncated, info = env.step(0)\n",
    "        observation = get_observation(frame)\n",
    "        state = torch.tensor(observation).float().to(device)\n",
    "\n",
    "        # Wait to have 4 frames as a first full sequence\n",
    "        for _ in range(3):\n",
    "            frame, reward, terminated, truncated, info = env.step(0)\n",
    "            observation = get_observation(frame)\n",
    "            state = torch.cat((state, torch.tensor(observation).float().to(device)))\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Decaying epsilon\n",
    "                epsilon = max(epsilon_min, epsilon - epsilon_decay)\n",
    "                if random.random() < epsilon:\n",
    "                    action = np.random.choice(list(action_map.keys()))\n",
    "                else:\n",
    "                    action = torch.argmax(Q(state)).item()\n",
    "\n",
    "            frame, reward, terminated, truncated, info = env.step(action_map[action])\n",
    "            observation = get_observation(frame)\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = state.clone()\n",
    "            next_state[:18] = next_state[6:].clone()\n",
    "            next_state[18:] = torch.tensor(observation).float().to(device)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        returns.append(total_reward)\n",
    "\n",
    "    return np.mean(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62c57e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_to_evaluate = [1750, 1800, 1850]\n",
    "# av_returns = {}\n",
    "\n",
    "# for checkpoint in checkpoints_to_evaluate:\n",
    "    \n",
    "#     training_vars = load_checkpoint(Q, Q_optimizer, Buffer, f\"checkpoints/training3/{checkpoint}.pth\")\n",
    "#     av_return = evaluate_average_return(Q, 10, 0)\n",
    "#     av_returns[checkpoint] = float(av_return)\n",
    "#     print(f\"{checkpoint} : {float(av_return)}\")\n",
    "\n",
    "# print(av_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1870ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count action taken\n",
    "\n",
    "# n_episodes = 10\n",
    "# epsilon = 0\n",
    "# actions = np.zeros(6)\n",
    "# for episode in range(n_episodes):\n",
    "\n",
    "#     # Skip first frame (different color)\n",
    "#     frame, _ = env.reset()\n",
    "#     _ = env.step(0)\n",
    "\n",
    "#     frame, reward, terminated, truncated, info = env.step(0)\n",
    "#     observation = get_observation(frame)\n",
    "#     state = torch.tensor(observation).float().to(device)\n",
    "\n",
    "#     # Wait to have 4 frames as a first full sequence\n",
    "#     for _ in range(3):\n",
    "#         frame, reward, terminated, truncated, info = env.step(0)\n",
    "#         observation = get_observation(frame)\n",
    "#         state = torch.cat((state, torch.tensor(observation).float().to(device)))\n",
    "\n",
    "#     done = False\n",
    "#     while not done:\n",
    "\n",
    "#         if np.random.rand() < epsilon:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             action = torch.argmax(Q(state)).item()\n",
    "\n",
    "#         actions[action] += 1\n",
    "#         frame, reward, terminated, truncated, info = env.step(action)\n",
    "#         observation = get_observation(frame)\n",
    "\n",
    "#         next_state = state.clone()\n",
    "#         next_state[:18] = next_state[6:].clone()\n",
    "#         next_state[18:] = torch.tensor(observation).float().to(device)\n",
    "\n",
    "#         done = terminated or truncated\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#         state = next_state\n",
    "\n",
    "# print(actions / np.sum(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "686bba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Pygame init ---\n",
    "# pygame.init()\n",
    "# screen = pygame.display.set_mode((400, 300))\n",
    "# pygame.display.set_caption(\"Play Pong with Keyboard\")\n",
    "# clock = pygame.time.Clock()\n",
    "\n",
    "# # --- Gym init ---\n",
    "# env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "# obs, info = env.reset()\n",
    "\n",
    "# # Mapping: keys -> actions\n",
    "# # 0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE\n",
    "# key_action_map = {\n",
    "#     pygame.K_UP: 2,     # Move up (RIGHT in Pong's terms)\n",
    "#     pygame.K_DOWN: 3,   # Move down (LEFT in Pong's terms)\n",
    "#     pygame.K_SPACE: 1,  # Fire (start the game)\n",
    "# }\n",
    "\n",
    "# done = False\n",
    "# while True:\n",
    "#     action = 0  # Default NOOP\n",
    "\n",
    "#     # --- Handle events ---\n",
    "#     for event in pygame.event.get():\n",
    "#         if event.type == pygame.QUIT:\n",
    "#             env.close()\n",
    "#             pygame.quit()\n",
    "#             raise SystemExit\n",
    "        \n",
    "#         if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:\n",
    "#             env.close()\n",
    "#             pygame.quit()\n",
    "#             raise SystemExit\n",
    "\n",
    "#     # Get pressed keys\n",
    "#     keys = pygame.key.get_pressed()\n",
    "#     for key, mapped_action in key_action_map.items():\n",
    "#         if keys[key]:\n",
    "#             action = mapped_action\n",
    "\n",
    "#     # Step the environment\n",
    "#     obs, reward, terminated, truncated, info = env.step(action)\n",
    "#     done = terminated or truncated\n",
    "\n",
    "#     if done:\n",
    "#         obs, info = env.reset()\n",
    "\n",
    "#     clock.tick(60)  # Limit loop to 60 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aae8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "\n",
    "# frame, info = env.reset()\n",
    "\n",
    "# observation = get_observation(frame)\n",
    "# state = torch.tensor(observation).float().to(device)\n",
    "\n",
    "# # Wait to have 4 first frames as a first full sequence\n",
    "# for _ in range(3):\n",
    "#     frame, reward, terminated, truncated, info = env.step(0)\n",
    "#     observation = get_observation(frame)\n",
    "#     state = torch.cat((state, torch.tensor(observation).float().to(device)))\n",
    "\n",
    "# for _ in range(10000):\n",
    "\n",
    "#     action = torch.argmax(Q(state)).item()\n",
    "#     frame, reward, terminated, truncated, info = env.step(action)\n",
    "#     observation = get_observation(frame)\n",
    "\n",
    "#     action = env.action_space.sample()  \n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     next_state = state.clone()\n",
    "#     next_state[:18] = next_state[6:].clone()\n",
    "#     next_state[18:] = torch.tensor(observation).float().to(device)\n",
    "\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#             break\n",
    "\n",
    "#     state = next_state\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589ab047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_colors = {tuple(map(int, pixel)) for pixel in state.reshape(-1, 3)}\n",
    "# print(unique_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d0d1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, actions, rewards, next_states, terminateds, truncateds = Buffer.sample(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacbae9",
   "metadata": {},
   "source": [
    "Inputs :\n",
    "- Can see the ball (bool)\n",
    "- Can see the adversary (bool)\n",
    "- x,y of the ball\n",
    "- y of both paddles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74a67a",
   "metadata": {},
   "source": [
    "Reward :\n",
    "- -1 if lost a point\n",
    "- +1 is won a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8888bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_state\n",
    "\n",
    "# frame, _ = env.reset()\n",
    "# ball_position = (0, np.array([0, 0]))\n",
    "# frames = []\n",
    "\n",
    "# for i in range(20):\n",
    "#     frame, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "#     ball_position, state = get_state(frame, ball_position)\n",
    "#     frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f49a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "# ax[0].imshow(frames[-2])\n",
    "# ax[1].imshow(frames[-1])\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "122421f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import utils\n",
    "\n",
    "# # Force reload\n",
    "# reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "251549a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import generate_evaluation_states\n",
    "# ev_states = generate_evaluation_states(env, device, 100)\n",
    "# torch.save(ev_states, \"ev_states.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64caeee2",
   "metadata": {},
   "source": [
    "# 1. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd7304",
   "metadata": {},
   "source": [
    "Blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d498aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training of start from scratch\n",
    "resume_training = False\n",
    "checkpoint = \"training/dqn/training3/10.pth\"\n",
    "\n",
    "max_training_time = 7 #h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f928065",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_numbers = [int(folder.split(\"training\")[-1]) for folder in glob.glob(\"training/dqn/*\")]\n",
    "training_number = max(training_numbers) + 1 if len(training_numbers) > 0 else 1\n",
    "os.mkdir(f\"training/dqn/training{training_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "386981a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import QNetwork, ReplayBuffer, Update_Q\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "Q = QNetwork(input_dim = 9, output_dim = 3).to(device)\n",
    "Q_target = QNetwork(input_dim = 9, output_dim = 3).to(device)\n",
    "Q_target.load_state_dict(Q.state_dict())\n",
    "Q_optimizer = torch.optim.Adam(Q.parameters(), lr = 0.0001)\n",
    "Buffer   = ReplayBuffer()\n",
    "\n",
    "ev_states = torch.load(\"ev_states.pt\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.00001\n",
    "batch_size = 32\n",
    "max_episode = 3000\n",
    "max_time_steps = 10000\n",
    "update_frequency = 1\n",
    "target_update_frequency = 1000\n",
    "checkpoint_frequency = 50\n",
    "ever_won = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc97f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import load_checkpoint\n",
    "\n",
    "if resume_training:\n",
    "    training_vars = load_checkpoint(Q, Q_optimizer, Buffer, checkpoint)\n",
    "    Q_target.load_state_dict(Q.state_dict())\n",
    "    returns, avg_Qvalues, td_losses, episode_start, epsilon = training_vars\n",
    "else:\n",
    "    episode_start = 0\n",
    "    returns = []\n",
    "    avg_Qvalues = []\n",
    "    td_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: -20.0\n",
      "episode: 1, reward: -20.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m Buffer\u001b[38;5;241m.\u001b[39mput([state, \u001b[38;5;28mint\u001b[39m(action), reward, next_state, terminated, truncated])\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Buffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m%\u001b[39m update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m         td_loss \u001b[38;5;241m=\u001b[39m \u001b[43mUpdate_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m         td_losses\u001b[38;5;241m.\u001b[39mappend(td_loss)\n\u001b[0;32m     46\u001b[0m tot_training_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\Coursera\\RL Specialization\\RL_applications\\Pong\\dqn.py:72\u001b[0m, in \u001b[0;36mUpdate_Q\u001b[1;34m(buffer, Q, Q_target, Q_optimizer, batch_size, gamma)\u001b[0m\n\u001b[0;32m     70\u001b[0m Q_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     71\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 72\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m Q_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:36\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:221\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m    216\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    218\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    219\u001b[0m         )\n\u001b[0;32m    220\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 221\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:36\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:89\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[1;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     87\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[0;32m     88\u001b[0m     ):\n\u001b[1;32m---> 89\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import get_state, generate_video\n",
    "from dqn import save_checkpoint, get_avg_Qvalues\n",
    "\n",
    "start_time = time.time()\n",
    "tot_training_steps = 0\n",
    "ball_position = (0, np.array([0, 0]))\n",
    "\n",
    "for episode in range(episode_start, max_episode):\n",
    "\n",
    "    total_reward = 0\n",
    "    points_scored = 0\n",
    "\n",
    "    # Skip first frame (different color)\n",
    "    frame, _ = env.reset()\n",
    "    _ = env.step(0)\n",
    "\n",
    "    frame, reward, terminated, truncated, info = env.step(0)\n",
    "    ball_position, state = get_state(frame, ball_position)\n",
    "    state = torch.tensor(state).float().to(device)\n",
    "\n",
    "    for t in range(max_time_steps):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Decaying epsilon\n",
    "            epsilon = max(epsilon_min, epsilon - epsilon_decay)\n",
    "            # epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = np.random.choice(list(action_map.keys()))\n",
    "            else:\n",
    "                action = torch.argmax(Q(state)).item()\n",
    "\n",
    "        frame, reward, terminated, truncated, info = env.step(action_map[action])\n",
    "\n",
    "        ball_position, next_state = get_state(frame, ball_position)\n",
    "        next_state = torch.tensor(next_state).float().to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward == 1 : points_scored += 1\n",
    "\n",
    "        Buffer.put([state, int(action), reward, next_state, terminated, truncated])\n",
    "\n",
    "        if Buffer.size() > 1000 and t % update_frequency == 0:\n",
    "                td_loss = Update_Q(Buffer, Q, Q_target, Q_optimizer, batch_size, gamma)\n",
    "                td_losses.append(td_loss)\n",
    "\n",
    "        tot_training_steps += 1\n",
    "        if tot_training_steps % target_update_frequency == 0:\n",
    "            Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "        if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # print('episode: {}, reward: {:.1f}'.format(episode, total_reward))\n",
    "    returns.append(total_reward)\n",
    "    avg_Qvalues.append(get_avg_Qvalues(Q, ev_states))\n",
    "\n",
    "    if (episode + 1) % 30 == 0:\n",
    "        print(f\"{episode+1} episodes done. Average reward on last 30 ep. : {np.mean(returns[-30:])}\")\n",
    "\n",
    "    if points_scored == 21 and not ever_won:\n",
    "        ever_won = True\n",
    "        print(f\"First win ! (Episode {episode})\")\n",
    "\n",
    "    # Training checkpoint\n",
    "    if (episode + 1) % checkpoint_frequency == 0:\n",
    "        save_checkpoint(Q, Q_optimizer, Buffer, returns, avg_Qvalues, td_losses, episode, epsilon, f\"training/dqn/training{training_number}/{episode+1}.pth\")\n",
    "        generate_video(env, Q, device, action_map, epsilon=0, n_episodes=1, filename=f\"training/dqn/training{training_number}/{episode+1}.mp4\")\n",
    "\n",
    "    if time.time() - start_time > 3600 * max_training_time:\n",
    "        print(f\"Maximum training time of {max_training_time}h exceeded. Interrupting training after {episode} episodes.\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d689e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_returns = [np.mean(returns[i-100:i]) for i in range(100, len(returns))]\n",
    "plt.plot(range(100, len(returns)), avg_returns)\n",
    "plt.title(\"Average return per episode (100 last episodes)\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(avg_Qvalues)), avg_Qvalues)\n",
    "plt.title(\"Average Q_value\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Q_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(td_losses)), td_losses)\n",
    "plt.title(\"TD Loss\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"TD Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
